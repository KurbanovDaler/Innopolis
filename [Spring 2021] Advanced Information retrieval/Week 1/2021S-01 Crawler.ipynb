{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "## 1. Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        response = requests.get(self.url)\n",
    "        if response.status_code == 200:\n",
    "            self.content = response.content\n",
    "            return True;        \n",
    "        return False\n",
    "    \n",
    "    def persist(self):\n",
    "        with open(quote(self.url).replace('/', '_'), 'wb') as file:\n",
    "            file.write(self.content)\n",
    "            \n",
    "    def load(self):\n",
    "        try:\n",
    "            with open(quote(self.url).replace('/', '_'), 'rb') as file:\n",
    "                self.content = file.read()            \n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse HTML ##\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "    \n",
    "    def parse(self):\n",
    "        soup = BeautifulSoup(self.content, 'lxml')\n",
    "        \n",
    "        self.anchors = []\n",
    "        self.images = []\n",
    "        self.text = \"\"\n",
    "        \n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            self.anchors.append((link.text, link.get('href')))\n",
    "        \n",
    "        imgs = soup.find_all('img')\n",
    "        for img in imgs:\n",
    "            img = urllib.parse.urljoin(self.url, img.get('src'))\n",
    "            self.images.append(img)\n",
    "        \n",
    "       \n",
    "        self.text = self.text_from_html(self.content)\n",
    "    \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def text_from_html(self, body):\n",
    "        soup = BeautifulSoup(body, 'html.parser')\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_texts = filter(self.tag_visible, texts)  \n",
    "        return u\" \".join(t.strip() for t in visible_texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"тестирующий сервер codetest\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/phone.png\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"http://university.innopolis.ru/\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). Your `get_word_stats()` method should return `Counter` object. Don't forget to lowercase your words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Romulus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Romulus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        #TODO*: implement sentence parser\n",
    "        result = sent_tokenize(self.doc.text, language = \"russian\")\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
    "        result = RegexpTokenizer(r'\\w+').tokenize(self.doc.text.lower())\n",
    "        result_withouth_sw = [word for word in result if not word in stopwords]\n",
    "\n",
    "        return Counter(result_withouth_sw)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('иннополис', 21), ('университета', 10), ('университет', 10), ('лаборатория', 10), ('университете', 7), ('области', 7), ('технологий', 7), ('робототехники', 7), ('12', 7), ('2020', 7)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис sould be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        queue = []\n",
    "        queue.append((source, 1))\n",
    "        visited = [source]\n",
    "        while(len(queue) > 0):\n",
    "            cur_url, cur_depth = queue.pop(0)\n",
    "            if(cur_depth > depth):\n",
    "                return\n",
    "            try:\n",
    "                HtmlDocument = HtmlDocumentTextData(cur_url)\n",
    "                for node in HtmlDocument.doc.anchors:\n",
    "                    node_url = node[1] \n",
    "                    if (node_url not in visited):\n",
    "                        visited.append(node_url)\n",
    "                        queue.append((node_url, cur_depth+1))\n",
    "                yield HtmlDocument\n",
    "            except: \n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/en/\n",
      "312 distinct word(s) so far\n",
      "http://old.innopolis.university/en/\n",
      "564 distinct word(s) so far\n",
      "https://media.innopolis.university/en/\n",
      "637 distinct word(s) so far\n",
      "https://www.facebook.com/InnopolisU\n",
      "701 distinct word(s) so far\n",
      "https://vk.com/innopolisu\n",
      "928 distinct word(s) so far\n",
      "https://www.youtube.com/user/InnopolisU\n",
      "952 distinct word(s) so far\n",
      "https://habr.com/ru/users/t-fazullin/posts/\n",
      "1653 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/\n",
      "2369 distinct word(s) so far\n",
      "https://corporate.innopolis.university/\n",
      "2636 distinct word(s) so far\n",
      "https://media.innopolis.university/\n",
      "2883 distinct word(s) so far\n",
      "https://innopolis.university/lk/\n",
      "3014 distinct word(s) so far\n",
      "https://career.innopolis.university/en/job/\n",
      "3139 distinct word(s) so far\n",
      "https://career.innopolis.university/en/\n",
      "3468 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/postgraduate-study/\n",
      "3631 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/bachelor/\n",
      "3707 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/master/\n",
      "3729 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/grant/\n",
      "3731 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/stud-life/\n",
      "3813 distinct word(s) so far\n",
      "https://innopolis.university/en/international-relations-office/\n",
      "4502 distinct word(s) so far\n",
      "https://innopolis.university/en/incomingstudents/\n",
      "4584 distinct word(s) so far\n",
      "https://innopolis.university/en/outgoingstudents/\n",
      "4619 distinct word(s) so far\n",
      "https://university.innopolis.ru/en/about/\n",
      "4685 distinct word(s) so far\n",
      "http://www.campuslife.innopolis.ru\n",
      "4773 distinct word(s) so far\n",
      "https://innopolis.university/en/research/\n",
      "4803 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/webinar-interstudents-eng/\n",
      "4822 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/devops-summer-school/\n",
      "4874 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/webinar-for-international-candidates-/\n",
      "4880 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/registration-innopolis-open-2020/\n",
      "4892 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/cyber-resilience-petrenko/\n",
      "4940 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/innopolis-university-extends-international-application-deadline-/\n",
      "4944 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/self-driven-school/\n",
      "4955 distinct word(s) so far\n",
      "https://apply.innopolis.ru/en/index.php\n",
      "5002 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/\n",
      "5002 distinct word(s) so far\n",
      "https://media.innopolis.university/en/events/\n",
      "5004 distinct word(s) so far\n",
      "http://www.minsvyaz.ru/en/\n",
      "5072 distinct word(s) so far\n",
      "Done\n",
      "[('the', 1337), ('of', 1212), ('and', 1196), ('in', 665), ('university', 647), ('to', 584), ('innopolis', 493), ('for', 456), ('a', 432), ('it', 262), ('is', 249), ('at', 220), ('i', 216), ('you', 200), ('with', 176), ('research', 175), ('on', 168), ('education', 164), ('students', 164), ('are', 157)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
