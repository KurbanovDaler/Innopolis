{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacka (midterm) thon \n",
    "\n",
    "## Detecting Malicious URLs \n",
    "\n",
    "Today you are invited to repeat the path of researchers Detecting Malicious URLs.\n",
    "An anonymized 120-day subset of our ICML-09 data set.\n",
    "The data set consists of about 2.4 million URLs (examples) and 3.2 million features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download data using link below\n",
    "[Download Dataset](http://www.sysnet.ucsd.edu/projects/url/url_svmlight.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Description of Data (SVM-light)\n",
    "Uncompressing the archive url_svmlight.tar.gz will yield a directory url_svmlight/ containing the following files:\n",
    "\n",
    "1. **FeatureTypes**. A text file list of feature indices that correspond to real-valued features.\n",
    "2. **DayX.svm** (where X is an integer from 0 to 120) --- The data for day X in SVM-light format. A label of +1 corresponds to a malicious URL and -1 corresponds to a benign URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Read article\n",
    "Please familiarize yourself with original research article. It will give you required context.\n",
    "\n",
    "*\"**Beyond Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs**\"* \n",
    "\n",
    "*Justin Ma, Lawrence K. Saul, Stefan Savage, Geoffrey M. Voelker* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 121 files\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "files = glob.glob('./url_svmlight/*.svm')\n",
    "print(\"There are %d files\" % len(files))\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting url_svmlight,f size 0\n",
      "extracting url_svmlight/Day33.svm,f size 18674876\n",
      "extracting url_svmlight/Day32.svm,f size 18599211\n",
      "extracting url_svmlight/Day53.svm,f size 18963938\n",
      "extracting url_svmlight/Day20.svm,f size 18633460\n",
      "extracting url_svmlight/Day7.svm,f size 18777054\n",
      "extracting url_svmlight/Day117.svm,f size 18106370\n",
      "max X = 3231952, max y dimension = 20000\n"
     ]
    }
   ],
   "source": [
    "uri = ('./url_svmlight.tar.gz')\n",
    "tar = tarfile.open(uri, \"r:gz\")\n",
    "max_obs = 0\n",
    "max_vars = 0\n",
    "i = 0\n",
    "split = 5\n",
    "for tarinfo in tar:\n",
    "    print(\"extracting %s,f size %s\" % (tarinfo.name, tarinfo.size))\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)\n",
    "        X,y = load_svmlight_file(f)\n",
    "        max_vars = np.maximum(max_vars, X.shape[0])\n",
    "        max_obs = np.maximum(max_obs, X.shape[1])\n",
    "    if i > split:\n",
    "        break\n",
    "    i+=1\n",
    "print(\"max X = %s, max y dimension = %s\" % (max_obs, max_vars)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.97      0.98     14567\n",
      "           1       0.92      0.97      0.95      5433\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.96      0.97      0.96     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "classes = [-1,1] # 1_:url- safety, -1: url- non-safety\n",
    "sgd = SGDClassifier(loss='log')\n",
    "n_features = 3231952\n",
    "split = 5\n",
    "i = 0\n",
    "for tarinfo in tar:\n",
    "    if i > split:\n",
    "        break\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)\n",
    "        X,y = load_svmlight_file(f,n_features=n_features)\n",
    "        if i < split:\n",
    "            sgd.partial_fit(X,y, classes = classes)\n",
    "        if i == split:\n",
    "            print (classification_report(sgd.predict(X),y))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading criteria\n",
    "- Complete solution - 60%\n",
    "- F1 Score - 40%\n",
    "    - The first 10 results get 40%\n",
    "    - Worst result get 20%\n",
    "    - All others are on a scale between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline\n",
    "20:00 MSK, April 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train, test\n",
    "- Upload data (you can use template above)\n",
    "- Separate your dataset into train and test subsets of observations\n",
    "- Use the 8:2 ratio: 80% train set, 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def train_test_split(train_size = 0.8):\n",
    "    \n",
    "    train = open('./url_svmlight/train.svm','w')\n",
    "    test  = open('./url_svmlight/test.svm','w')\n",
    "    summ = 0\n",
    "    for i in range(121):\n",
    "        inn = open('./url_svmlight/Day' + str(i) + '.svm','r')\n",
    "        \n",
    "        print(\"file: \" + str(i))\n",
    "        \n",
    "        q = [0, 0]\n",
    "        \n",
    "        for line in inn:\n",
    "            a = line.split()\n",
    "            if(a[0] == \"-1\"):\n",
    "                q[0] += 1\n",
    "            else:\n",
    "                q[1] += 1\n",
    "        \n",
    "        inn.close()\n",
    "        summ += q[0] + q[1]\n",
    "        check = [int(train_size * q[0]),int(train_size * q[1])]\n",
    "        start = [0, 0]\n",
    "        \n",
    "        inn = open('./url_svmlight/Day' + str(i) + '.svm','r')\n",
    "        \n",
    "        if (train_size * q[0]) % 1 >=0.5:\n",
    "            check[0] += 1\n",
    "        if (train_size * q[1]) % 1 >=0.5:\n",
    "            check[1] += 1\n",
    "        for line in inn:\n",
    "            a = line.split()\n",
    "\n",
    "            rand = random()\n",
    "            if rand > 0.5:\n",
    "                if a[0] == \"-1\":\n",
    "                    if start[0] + 1 <= check[0]:\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[0] += 1\n",
    "                    else:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[0] -= 1\n",
    "                else:\n",
    "                    if start[1] + 1 <= check[1]:\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[1] += 1\n",
    "                    else:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[1] -= 1\n",
    "            else:\n",
    "                if(a[0] == \"-1\"):\n",
    "                    if q[0] > check[0]:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[0] -= 1\n",
    "                    else :\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[0] += 1\n",
    "                else:\n",
    "                    if q[1] > check[1]:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[1] -= 1\n",
    "                    else :\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[1] += 1\n",
    "        print(\"finish: \" + str(i))\n",
    "    print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0\n",
      "finish: 0\n",
      "file: 1\n",
      "finish: 1\n",
      "file: 2\n",
      "finish: 2\n",
      "file: 3\n",
      "finish: 3\n",
      "file: 4\n",
      "finish: 4\n",
      "file: 5\n",
      "finish: 5\n",
      "file: 6\n",
      "finish: 6\n",
      "file: 7\n",
      "finish: 7\n",
      "file: 8\n",
      "finish: 8\n",
      "file: 9\n",
      "finish: 9\n",
      "file: 10\n",
      "finish: 10\n",
      "file: 11\n",
      "finish: 11\n",
      "file: 12\n",
      "finish: 12\n",
      "file: 13\n",
      "finish: 13\n",
      "file: 14\n",
      "finish: 14\n",
      "file: 15\n",
      "finish: 15\n",
      "file: 16\n",
      "finish: 16\n",
      "file: 17\n",
      "finish: 17\n",
      "file: 18\n",
      "finish: 18\n",
      "file: 19\n",
      "finish: 19\n",
      "file: 20\n",
      "finish: 20\n",
      "file: 21\n",
      "finish: 21\n",
      "file: 22\n",
      "finish: 22\n",
      "file: 23\n",
      "finish: 23\n",
      "file: 24\n",
      "finish: 24\n",
      "file: 25\n",
      "finish: 25\n",
      "file: 26\n",
      "finish: 26\n",
      "file: 27\n",
      "finish: 27\n",
      "file: 28\n",
      "finish: 28\n",
      "file: 29\n",
      "finish: 29\n",
      "file: 30\n",
      "finish: 30\n",
      "file: 31\n",
      "finish: 31\n",
      "file: 32\n",
      "finish: 32\n",
      "file: 33\n",
      "finish: 33\n",
      "file: 34\n",
      "finish: 34\n",
      "file: 35\n",
      "finish: 35\n",
      "file: 36\n",
      "finish: 36\n",
      "file: 37\n",
      "finish: 37\n",
      "file: 38\n",
      "finish: 38\n",
      "file: 39\n",
      "finish: 39\n",
      "file: 40\n",
      "finish: 40\n",
      "file: 41\n",
      "finish: 41\n",
      "file: 42\n",
      "finish: 42\n",
      "file: 43\n",
      "finish: 43\n",
      "file: 44\n",
      "finish: 44\n",
      "file: 45\n",
      "finish: 45\n",
      "file: 46\n",
      "finish: 46\n",
      "file: 47\n",
      "finish: 47\n",
      "file: 48\n",
      "finish: 48\n",
      "file: 49\n",
      "finish: 49\n",
      "file: 50\n",
      "finish: 50\n",
      "file: 51\n",
      "finish: 51\n",
      "file: 52\n",
      "finish: 52\n",
      "file: 53\n",
      "finish: 53\n",
      "file: 54\n",
      "finish: 54\n",
      "file: 55\n",
      "finish: 55\n",
      "file: 56\n",
      "finish: 56\n",
      "file: 57\n",
      "finish: 57\n",
      "file: 58\n",
      "finish: 58\n",
      "file: 59\n",
      "finish: 59\n",
      "file: 60\n",
      "finish: 60\n",
      "file: 61\n",
      "finish: 61\n",
      "file: 62\n",
      "finish: 62\n",
      "file: 63\n",
      "finish: 63\n",
      "file: 64\n",
      "finish: 64\n",
      "file: 65\n",
      "finish: 65\n",
      "file: 66\n",
      "finish: 66\n",
      "file: 67\n",
      "finish: 67\n",
      "file: 68\n",
      "finish: 68\n",
      "file: 69\n",
      "finish: 69\n",
      "file: 70\n",
      "finish: 70\n",
      "file: 71\n",
      "finish: 71\n",
      "file: 72\n",
      "finish: 72\n",
      "file: 73\n",
      "finish: 73\n",
      "file: 74\n",
      "finish: 74\n",
      "file: 75\n",
      "finish: 75\n",
      "file: 76\n",
      "finish: 76\n",
      "file: 77\n",
      "finish: 77\n",
      "file: 78\n",
      "finish: 78\n",
      "file: 79\n",
      "finish: 79\n",
      "file: 80\n",
      "finish: 80\n",
      "file: 81\n",
      "finish: 81\n",
      "file: 82\n",
      "finish: 82\n",
      "file: 83\n",
      "finish: 83\n",
      "file: 84\n",
      "finish: 84\n",
      "file: 85\n",
      "finish: 85\n",
      "file: 86\n",
      "finish: 86\n",
      "file: 87\n",
      "finish: 87\n",
      "file: 88\n",
      "finish: 88\n",
      "file: 89\n",
      "finish: 89\n",
      "file: 90\n",
      "finish: 90\n",
      "file: 91\n",
      "finish: 91\n",
      "file: 92\n",
      "finish: 92\n",
      "file: 93\n",
      "finish: 93\n",
      "file: 94\n",
      "finish: 94\n",
      "file: 95\n",
      "finish: 95\n",
      "file: 96\n",
      "finish: 96\n",
      "file: 97\n",
      "finish: 97\n",
      "file: 98\n",
      "finish: 98\n",
      "file: 99\n",
      "finish: 99\n",
      "file: 100\n",
      "finish: 100\n",
      "file: 101\n",
      "finish: 101\n",
      "file: 102\n",
      "finish: 102\n",
      "file: 103\n",
      "finish: 103\n",
      "file: 104\n",
      "finish: 104\n",
      "file: 105\n",
      "finish: 105\n",
      "file: 106\n",
      "finish: 106\n",
      "file: 107\n",
      "finish: 107\n",
      "file: 108\n",
      "finish: 108\n",
      "file: 109\n",
      "finish: 109\n",
      "file: 110\n",
      "finish: 110\n",
      "file: 111\n",
      "finish: 111\n",
      "file: 112\n",
      "finish: 112\n",
      "file: 113\n",
      "finish: 113\n",
      "file: 114\n",
      "finish: 114\n",
      "file: 115\n",
      "finish: 115\n",
      "file: 116\n",
      "finish: 116\n",
      "file: 117\n",
      "finish: 117\n",
      "file: 118\n",
      "finish: 118\n",
      "file: 119\n",
      "finish: 119\n",
      "file: 120\n",
      "finish: 120\n",
      "2396130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "train_test_split()\n",
    "\n",
    "data = None\n",
    "n_features = 3231961\n",
    "\n",
    "\n",
    "data, target = load_svmlight_file(\"./url_svmlight/train.svm\",n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3231961\n",
    "data, target = load_svmlight_file(\"./url_svmlight/train.svm\",n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1916904, 3231961)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Find out whether it is possible to reduce the dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01891587 0.10121791 0.05826583 0.02598372 0.02237064 0.02065406\n",
      " 0.02027658 0.01812224 0.01594569 0.01300335 0.01200047 0.01116721\n",
      " 0.01082642 0.01018591 0.0098815  0.00910171 0.00869914 0.00782822\n",
      " 0.0074706  0.00727178 0.00677976 0.00632283 0.00625342 0.00601369\n",
      " 0.00575902 0.00536761 0.00510548 0.00500917 0.00479991 0.00429118\n",
      " 0.00411876 0.00402973 0.00378781 0.00356946 0.00340468 0.00325698\n",
      " 0.00319533 0.00316682 0.00315137 0.0029938  0.00287629 0.00284309\n",
      " 0.002793   0.00273524 0.00273223 0.00265768 0.00264986 0.00259308\n",
      " 0.00254209 0.00246111 0.00243054 0.00236821 0.00231311 0.00227587\n",
      " 0.00222463 0.00215692 0.00212117 0.00208857 0.00206884 0.00202759\n",
      " 0.0019657  0.00190219 0.00189828 0.00184138 0.00183582 0.00176562\n",
      " 0.00172946 0.00169214 0.00166553 0.00160721 0.00159657 0.0015808\n",
      " 0.00152448 0.00151846 0.00150706 0.00148017 0.00146664 0.00142565\n",
      " 0.0014191  0.00141386 0.00139357 0.00138097 0.00134218 0.00132144\n",
      " 0.00128823 0.00127778 0.00125667 0.00124371 0.00122603 0.00119781\n",
      " 0.00118954 0.00116627 0.00114382 0.00114031 0.00111927 0.00111072\n",
      " 0.00109257 0.00106931 0.00106157 0.00105027]\n",
      "0.6054329610594393\n",
      "[11461.61623486  2701.90583906  2054.41002201  1368.96067678\n",
      "  1270.17038339  1220.49897012  1209.30285264  1143.21881635\n",
      "  1072.37245137   968.39325678   930.29871931   897.5160589\n",
      "   883.62208536   857.12548729   844.18188547   810.21121159\n",
      "   792.07200089   751.39692874   734.01269981   724.17465714\n",
      "   699.30667982   675.296912     671.55559465   658.56617781\n",
      "   644.47211522   622.23123101   606.80764378   601.04676607\n",
      "   588.51988523   556.30390047   545.01740346   539.09050565\n",
      "   522.67501289   507.40530247   495.55883751   484.72301351\n",
      "   480.09402728   477.93089788   476.73122604   464.67545264\n",
      "   455.4571328    452.85882871   448.80749048   444.14139423\n",
      "   443.90558067   437.8393661    437.17424402   432.44641418\n",
      "   428.17747231   421.3095732    418.6793237    413.27069235\n",
      "   408.43699555   405.13721997   400.55061529   394.40318552\n",
      "   391.12077212   388.11538191   386.27204589   382.40407621\n",
      "   376.51512512   370.38588522   370.01326155   364.42205959\n",
      "   363.87794362   356.83845832   353.16710078   349.3416769\n",
      "   346.57614431   340.45672315   339.32633312   337.64592711\n",
      "   331.57730165   330.92113719   329.67728588   326.72506764\n",
      "   325.22951201   320.65409326   319.91175548   319.32122485\n",
      "   317.02022592   315.58369154   311.12071796   308.70679828\n",
      "   304.80406928   303.5660698    301.04641805   299.49755253\n",
      "   297.35693475   293.91263951   292.89620597   290.01623516\n",
      "   287.21183228   286.77078518   284.11259103   283.02624304\n",
      "   280.70377781   277.69996334   276.69560549   275.21654733]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Yes, we could. However, the initial paper proposed online learning instead of batch. Moreover, it does not contain mentions of dimensionality reduction.\n",
    "# However, since we are using batch learning we might want to do that to reduce dimensionality. For instance, soft confidence weighted algorithm cannot perform \n",
    "# computations on such a large dataset.\n",
    "# Comments: We reduced the initial dataset to 100 features using Truncated SVD. It captures 60.5% of explained variance ratio.\n",
    "# Truncated SVD should perform better on large sparse datasets which cannot be centered without making the memory usage explode.\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=0)\n",
    "svd.fit(data)\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_transform = svd.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1916904, 100)\n"
     ]
    }
   ],
   "source": [
    "print(svd_transform.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scw import SCW1, SCW2\n",
    "import numpy as np\n",
    "# f = np.array(X)\n",
    "# print(f)\n",
    "# print(np.ndim(f))\n",
    "scw = SCW1(C=1.0, ETA=1.0)\n",
    "scw.fit(svd_transform, target)\n",
    "y_pred = scw.predict(svd_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get the quality\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1   0.978132  0.973317  0.975719    322382\n",
      "           1   0.945705  0.955274  0.950465    156844\n",
      "\n",
      "    accuracy                       0.967412    479226\n",
      "   macro avg   0.961919  0.964296  0.963092    479226\n",
      "weighted avg   0.967519  0.967412  0.967454    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data, target = load_svmlight_file(\"./url_svmlight/test.svm\",n_features=n_features)\n",
    "test_svd_transform = svd.transform(data)\n",
    "print (classification_report(scw.predict(test_svd_transform),target, digits = 6))\n",
    "# test_svd_transform = svd.transform(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
