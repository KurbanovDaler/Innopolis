{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab-11.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PZKrlwajuGZ-"},"source":["# Lab-13: Ensemble Learning\n","\n","In this lab, we will look at different ways to build ensemble models.\n","\n","\n","## Objectives:\n","\n","* Bagging\n","* Random Forests\n","* AdaBoost\n","\n","\n","Why ensemble learning? How does it help?"]},{"cell_type":"markdown","metadata":{"id":"4G2NXb2yznzd"},"source":["## Ensemble learning\n","We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n","\n","Let's start from uploading MNIST dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"9sI82NDtzoSP","executionInfo":{"status":"ok","timestamp":1606808689412,"user_tz":-360,"elapsed":2462,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"88098a46-84c9-4148-ed4c-525d5e71df61"},"source":["from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","digits = load_digits()\n","X = digits.data\n","y = digits.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n","\n","\n","plt.figure(1, figsize=(3, 3))\n","plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n","plt.xticks([])\n","plt.yticks([])\n","plt.title(f\"label is {y[0]}\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 216x216 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"TcJSouftKwUC"},"source":["### Single decision tree\n","\n","First, we train a single decision tree."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqrQbHVDKw9F","executionInfo":{"status":"ok","timestamp":1606808689413,"user_tz":-360,"elapsed":2457,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"21e701f1-1296-415f-fc21-c8dcbe35429b"},"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","tree = DecisionTreeClassifier()\n","tree.fit(X_train, y_train)\n","pred = tree.predict(X_test)\n","tree_score = accuracy_score(y_test, pred)\n","print(\"Single tree accuracy:\", tree_score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Single tree accuracy: 0.8703703703703703\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qgAj4l5GLKuG"},"source":["Note the accuracy - it is around **0.85**.\n","\n","### Bagging\n","\n","\n","What is decreased by bagging? Variance or bias? How? \n","\n","Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWQTCYrGLLMM","executionInfo":{"status":"ok","timestamp":1606808941522,"user_tz":-360,"elapsed":2571,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"d5ae04c9-8789-415f-ffbf-cbbf4ba1be42"},"source":["import numpy as np\n","from scipy.stats import mode\n","\n","n_trees = 100\n","\n","classifiers = []\n","for i in range(n_trees):\n","    # train a new classifier and append it to the list\n","    classifiers.append(DecisionTreeClassifier().fit(X_train, y_train))\n","    pass\n","\n","\n","# here we will store predictions for all samples and all base classifiers\n","base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n","for i in range(n_trees):\n","    # obtain the predictions from each tree\n","    base_pred[:,i] = classifiers[i].predict(X_test)\n","\n","print(base_pred)\n","\n","# aggregate predictions by majority voting\n","pred = mode(base_pred, axis=1)[0].ravel()\n","acc = accuracy_score(y_test, pred)\n","print(\"Bagging accuracy:\", acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[3 3 3 ... 3 3 8]\n"," [8 8 8 ... 8 8 8]\n"," [2 2 2 ... 2 2 2]\n"," ...\n"," [1 1 1 ... 1 1 1]\n"," [1 1 1 ... 1 1 1]\n"," [2 2 2 ... 2 2 2]]\n","Bagging accuracy: 0.8838383838383839\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ODOPvJJ2bKPh"},"source":["Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all?\n","\n","\n","### Random forest\n","\n","Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n","\n","At first, let's implement bootstrap sampling."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9d1ElFpE48c","executionInfo":{"status":"ok","timestamp":1606810216225,"user_tz":-360,"elapsed":687,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"3c2c4ae5-5e3e-4049-dc94-53171fb39de7"},"source":["\n","\n","def bootstrap(X, y):\n","    # generate bootstrap indices and return data according to them\n","    indices = np.random.randint(len(X), size = len(X))\n","    X_bs = X[indices]\n","    y_bs = y[indices]\n","    \n","\n","\n","    return X_bs, y_bs\n","\n","\n","# this is a test, will work if you are using np.random.randint() for indices generation\n","np.random.seed(0)\n","a = np.array(range(12)).reshape(4,3)\n","b = np.array(range(4))\n","bootstrap(a, b)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 0,  1,  2],\n","        [ 9, 10, 11],\n","        [ 3,  4,  5],\n","        [ 0,  1,  2]]), array([0, 3, 1, 0]))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"NxpCck94Y73A"},"source":["You should get\n","\n","(array([[ 0,  1,  2], <br>\n","&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n","&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n","&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n","array([0, 3, 1, 0]))\n","       \n","Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxIhmI_H5jnI","executionInfo":{"status":"ok","timestamp":1606810626409,"user_tz":-360,"elapsed":1310,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"af4b89bb-7b41-498a-dd1b-a7cd95b7e6f3"},"source":["classifiers = []\n","for i in range(n_trees):\n","    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list    \n","    X_bootstrapped, y_bootstrapped = bootstrap(X_train, y_train)\n","    \n","    tree = DecisionTreeClassifier(max_features = 'sqrt')\n","    \n","    tree.fit(X_bootstrapped, y_bootstrapped)\n","    classifiers.append(tree)\n","\n","    pass\n","\n","base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n","for i in range(n_trees):\n","    base_pred[i,:] = classifiers[i].predict(X_test)\n","\n","pred = mode(base_pred, axis=0)[0].ravel()\n","acc = accuracy_score(y_test, pred)\n","print(\"Random forest accuracy:\", acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random forest accuracy: 0.9797979797979798\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ObGBbQfrE5jM"},"source":["And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."]},{"cell_type":"markdown","metadata":{"id":"qp4WWdYezpHy"},"source":["## Boosting\n","\n","How does boosting work? \n","\n","For simplicity let's make a binary classification problem out of the original problem."]},{"cell_type":"code","metadata":{"id":"JBbO7p-aNk69"},"source":["y_train_b = (y_train == 2 ) * 2 - 1\n","y_test_b = (y_test == 2 ) * 2 - 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"talvfivTQitE"},"source":["Now let's train a boosting model.\n","\n","We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n","\n","Tree weight $w$ is computed using weighted error or $1 - accuracy$\n","\n","$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n","\n","For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVEBRi4pEwte","executionInfo":{"status":"ok","timestamp":1606812464082,"user_tz":-360,"elapsed":734,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"e1a84738-5d8f-4d20-d8d2-62fa89f05580"},"source":["n_trees = 10\n","tree_weights = np.zeros(n_trees)\n","classifiers = []\n","train_samples = X_train.shape[0]\n","# initialize sample weights\n","sample_weights = np.ones(shape = train_samples)\n","for i in range(n_trees):\n","    \n","    clf = DecisionTreeClassifier(min_samples_leaf=3)\n","    \n","    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n","    \n","    pred = clf.predict(X_train)\n","    \n","    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n","    \n","    # caclulate tree weight              \n","\n","    w = np.log((1 - acc) / acc) / 2\n","\n","    print(w)\n","    print(acc)\n","\n","    tree_weights[i] = w\n","    \n","    classifiers.append(clf)\n","\n","    # update sample weights\n","\n","    for j in range(train_samples):\n","        if pred[j] != y[j]:\n","            sample_weights[j] *= np.exp(w)            \n","        else:\n","            # in case when the prediction is correct\n","            sample_weights[j] *= np.exp(-w)            \n","    # normalize the weights\n","    \n","    \n","\n","    # new_sample_weights = (\n","    #         sample_weights * np.exp(-w)\n","    # )\n","        \n","    # new_sample_weights /= new_sample_weights.sum()\n","\n","    sample_weights = sample_weights / sample_weights.sum()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-2.739485433120647\n","0.9958437240232751\n","-3.575298438480278\n","0.9992162190406519\n","-6.917004064634258\n","0.9999990183285049\n","-10.787893197587563\n","0.999999999573661\n","-11.454685167027689\n","0.9999999998876464\n","-11.454685167027689\n","0.9999999998876464\n","-11.454685167027689\n","0.9999999998876464\n","-11.454685167027689\n","0.9999999998876464\n","-11.454685167027689\n","0.9999999998876464\n","-11.454685167027689\n","0.9999999998876464\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YJ43LIorSXbs"},"source":["Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n","\n","$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpJKjCDdzpmt","executionInfo":{"status":"ok","timestamp":1606812515098,"user_tz":-360,"elapsed":731,"user":{"displayName":"Daler Kurbanov","photoUrl":"https://lh5.googleusercontent.com/-NtIAZcVbMqE/AAAAAAAAAAI/AAAAAAAAFLw/52c4k2vVgWc/s64/photo.jpg","userId":"17027517711886510863"}},"outputId":"a7fcf963-22ff-4ef7-a517-2601cfaf5ad5"},"source":["n_test = X_test.shape[0]\n","print(n_test)\n","pred = np.zeros(n_test)\n","# caclulate predictions\n","for i in range(n_trees):\n","  pred += classifiers[i].predict(X_test) * tree_weights[i]\n","\n","for i in range(n_test):\n","  pred[i] = 1 if pred[i] > 0 else -1\n","acc = accuracy_score(y_test_b, pred)\n","print(\"Boosting accuracy:\", acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["594\n","Boosting accuracy: 0.8956228956228957\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E7mv1TfwSahW"},"source":["The resulting accuracy is **0.97**"]}]}